{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe6da793-5c83-408b-b117-cc4dc78e3395",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ec56c-943e-437d-bb76-3ed8c6b006c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Phase I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1d6f0-22f8-4c60-aee1-c3e07e0e76b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 search_engine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "352d00c4-9623-4831-b1c0-0e7330a56ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Rich Dataset...\n",
      "üöÄ Initializing Search Engine on cuda...\n",
      "   üìÇ Found cached index 'data/search_index.pt'. Loading...\n",
      "   ‚úÖ Loaded from disk successfully.\n",
      "\n",
      "Generatng Golden Set (10 queries)...\n",
      "   üîç Query: Women's floral embroidery slip-on shoes\n",
      "   üîç Query: Men's leather oxford dress shoes black\n",
      "   üîç Query: Bohemian style multi-colored rug\n",
      "   üîç Query: Industrial style metal pendant light\n",
      "   üîç Query: Vintage brass drawer handles\n",
      "   üîç Query: Running shoes with breathable mesh\n",
      "   üîç Query: Tufted velvet chesterfield sofa\n",
      "   üîç Query: Geometric print throw pillow blue\n",
      "   üîç Query: Ceramic vase modern white\n",
      "   üîç Query: Women's high heel sandals gladiator style\n",
      "\n",
      "‚úÖ Repository saved to data/query_repository.json\n"
     ]
    }
   ],
   "source": [
    "!python3 queries_repository.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d136b09f-3434-48f7-88a8-8d54f593f83f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading llava-hf/llava-1.5-13b-hf to GPU...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 43.95it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "‚úÖ Model Online.\n",
      "Total Images: 99\n",
      "Remaining:    99\n",
      "Extracting Attributes:   0%|                             | 0/99 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Extracting Attributes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [05:32<00:00,  3.36s/it]\n",
      "\n",
      "üéâ Extraction Complete. Saved to data/dense_captions.json\n"
     ]
    }
   ],
   "source": [
    "!python3 visual_extractor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7197b045-8d46-4cdd-b030-a6e8a7851a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Two-Step Simulation on 10 queries...\n",
      "\n",
      "--- Simulation 1: 'Women's floral embroidery slip-on shoes' ---\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "Connection error while contacting Ollama: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x752b9ab0c860>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Ollama server seems down. Attempting to start it...\n",
      "time=2025-12-11T13:00:51.741+05:30 level=INFO source=routes.go:1544 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:4 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/ajit_2411ai19/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2025-12-11T13:00:51.742+05:30 level=INFO source=images.go:522 msg=\"total blobs: 5\"\n",
      "time=2025-12-11T13:00:51.743+05:30 level=INFO source=images.go:529 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-12-11T13:00:51.743+05:30 level=INFO source=routes.go:1597 msg=\"Listening on 127.0.0.1:11434 (version 0.12.11)\"\n",
      "time=2025-12-11T13:00:51.744+05:30 level=INFO source=runner.go:67 msg=\"discovering available GPUs...\"\n",
      "time=2025-12-11T13:00:51.745+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 41753\"\n",
      "time=2025-12-11T13:00:52.112+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 36325\"\n",
      "time=2025-12-11T13:00:52.278+05:30 level=INFO source=types.go:42 msg=\"inference compute\" id=GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 filter_id=\"\" library=CUDA compute=8.0 name=CUDA0 description=\"NVIDIA A100-PCIE-40GB\" libdirs=ollama,cuda_v12 driver=12.2 pci_id=0000:e1:00.0 type=discrete total=\"40.0 GiB\" available=\"39.4 GiB\"\n",
      "Attempt 2/8 to call Ollama...\n",
      "time=2025-12-11T13:01:02.156+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 33389\"\n",
      "time=2025-12-11T13:01:02.841+05:30 level=INFO source=server.go:209 msg=\"enabling flash attention\"\n",
      "time=2025-12-11T13:01:02.842+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --model /home/ajit_2411ai19/.ollama/models/blobs/sha256-e7b273f9636059a689e3ddcab3716e4f65abe0143ac978e46673ad0e52d09efb --port 34015\"\n",
      "time=2025-12-11T13:01:02.843+05:30 level=INFO source=sched.go:443 msg=\"system memory\" total=\"251.7 GiB\" free=\"215.0 GiB\" free_swap=\"9.3 GiB\"\n",
      "time=2025-12-11T13:01:02.843+05:30 level=INFO source=sched.go:450 msg=\"gpu memory\" id=GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 library=CUDA available=\"38.9 GiB\" free=\"39.4 GiB\" minimum=\"457.0 MiB\" overhead=\"0 B\"\n",
      "time=2025-12-11T13:01:02.843+05:30 level=INFO source=server.go:702 msg=\"loading model\" \"model layers\"=25 requested=-1\n",
      "time=2025-12-11T13:01:02.873+05:30 level=INFO source=runner.go:1398 msg=\"starting ollama engine\"\n",
      "time=2025-12-11T13:01:02.873+05:30 level=INFO source=runner.go:1433 msg=\"Server listening on 127.0.0.1:34015\"\n",
      "time=2025-12-11T13:01:02.879+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T13:01:03.004+05:30 level=INFO source=ggml.go:136 msg=\"\" architecture=gptoss file_type=MXFP4 name=\"\" description=\"\" num_tensors=459 num_key_values=32\n",
      "load_backend: loaded CPU backend from /home/ajit_2411ai19/opt/ollama/lib/ollama/libggml-cpu-haswell.so\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100-PCIE-40GB, compute capability 8.0, VMM: yes, ID: GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14\n",
      "load_backend: loaded CUDA backend from /home/ajit_2411ai19/opt/ollama/lib/ollama/cuda_v12/libggml-cuda.so\n",
      "time=2025-12-11T13:01:03.226+05:30 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-12-11T13:01:04.026+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T13:01:04.263+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T13:01:04.264+05:30 level=INFO source=ggml.go:482 msg=\"offloading 24 repeating layers to GPU\"\n",
      "time=2025-12-11T13:01:04.264+05:30 level=INFO source=ggml.go:489 msg=\"offloading output layer to GPU\"\n",
      "time=2025-12-11T13:01:04.264+05:30 level=INFO source=ggml.go:494 msg=\"offloaded 25/25 layers to GPU\"\n",
      "time=2025-12-11T13:01:04.264+05:30 level=INFO source=device.go:240 msg=\"model weights\" device=CUDA0 size=\"11.8 GiB\"\n",
      "time=2025-12-11T13:01:04.264+05:30 level=INFO source=device.go:245 msg=\"model weights\" device=CPU size=\"1.1 GiB\"\n",
      "time=2025-12-11T13:01:04.264+05:30 level=INFO source=device.go:251 msg=\"kv cache\" device=CUDA0 size=\"300.0 MiB\"\n",
      "time=2025-12-11T13:01:04.264+05:30 level=INFO source=device.go:262 msg=\"compute graph\" device=CUDA0 size=\"178.6 MiB\"\n",
      "time=2025-12-11T13:01:04.264+05:30 level=INFO source=device.go:267 msg=\"compute graph\" device=CPU size=\"5.6 MiB\"\n",
      "time=2025-12-11T13:01:04.264+05:30 level=INFO source=device.go:272 msg=\"total memory\" size=\"13.3 GiB\"\n",
      "time=2025-12-11T13:01:04.264+05:30 level=INFO source=sched.go:517 msg=\"loaded runners\" count=1\n",
      "time=2025-12-11T13:01:04.264+05:30 level=INFO source=server.go:1294 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-12-11T13:01:04.265+05:30 level=INFO source=server.go:1328 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "time=2025-12-11T13:01:07.530+05:30 level=INFO source=server.go:1332 msg=\"llama runner started in 4.69 seconds\"\n",
      "[GIN] 2025/12/11 - 13:01:38 |\u001b[97;42m 200 \u001b[0m| 36.640343397s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:01:51 |\u001b[97;42m 200 \u001b[0m| 12.661536879s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   ü•á Top Visible: B071G16W12 (Score: 3.2492)\n",
      "\n",
      "--- Simulation 2: 'Men's leather oxford dress shoes black' ---\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:02:19 |\u001b[97;42m 200 \u001b[0m| 28.320543235s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:02:35 |\u001b[97;42m 200 \u001b[0m| 15.818882291s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   ü•á Top Visible: B07QDB9V9X (Score: 0.9419)\n",
      "\n",
      "--- Simulation 3: 'Bohemian style multi-colored rug' ---\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:03:16 |\u001b[97;42m 200 \u001b[0m| 41.018228107s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:03:34 |\u001b[97;42m 200 \u001b[0m|  18.79323797s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   ü•á Top Visible: B07QFB4XBP (Score: 0.1294)\n",
      "\n",
      "--- Simulation 4: 'Industrial style metal pendant light' ---\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:04:14 |\u001b[97;42m 200 \u001b[0m| 39.734150585s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:04:29 |\u001b[97;42m 200 \u001b[0m| 14.670014043s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   ü•á Top Visible: B07HKGHSJJ (Score: 1.3485)\n",
      "\n",
      "--- Simulation 5: 'Vintage brass drawer handles' ---\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:04:57 |\u001b[97;42m 200 \u001b[0m| 28.337465273s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:05:16 |\u001b[97;42m 200 \u001b[0m| 18.408534145s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   ü•á Top Visible: B07GDQM2TW (Score: 4.2481)\n",
      "\n",
      "--- Simulation 6: 'Running shoes with breathable mesh' ---\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:05:48 |\u001b[97;42m 200 \u001b[0m| 32.376676628s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:06:11 |\u001b[97;42m 200 \u001b[0m|  22.64244072s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   ü•á Top Visible: B07R7BHNF4 (Score: 0.9608)\n",
      "\n",
      "--- Simulation 7: 'Tufted velvet chesterfield sofa' ---\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:06:43 |\u001b[97;42m 200 \u001b[0m| 32.684366038s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:07:02 |\u001b[97;42m 200 \u001b[0m| 18.143278833s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   ü•á Top Visible: B07CB54MM3 (Score: 0.7165)\n",
      "\n",
      "--- Simulation 8: 'Geometric print throw pillow blue' ---\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:07:27 |\u001b[97;42m 200 \u001b[0m| 25.315828383s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:07:40 |\u001b[97;42m 200 \u001b[0m| 13.465824587s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   ü•á Top Visible: B07MF1RKS9 (Score: 0.8421)\n",
      "\n",
      "--- Simulation 9: 'Ceramic vase modern white' ---\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:08:03 |\u001b[97;42m 200 \u001b[0m| 23.186381208s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:08:16 |\u001b[97;42m 200 \u001b[0m| 12.129332857s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   ü•á Top Visible: B078JGHZT3 (Score: 0.3319)\n",
      "\n",
      "--- Simulation 10: 'Women's high heel sandals gladiator style' ---\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:08:46 |\u001b[97;42m 200 \u001b[0m| 30.435317093s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 13:09:05 |\u001b[97;42m 200 \u001b[0m| 18.567323689s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   ü•á Top Visible: B07894CL1P (Score: 0.0)\n",
      "\n",
      "‚úÖ Simulation Complete. Logs saved to data/simulation_logs.json\n"
     ]
    }
   ],
   "source": [
    "!python3 run_simulator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f77e2136-4a24-43d6-8921-0ea7cc18f705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading Full 7k Dataset for Brand Analysis...\n",
      "   Scanning 7084 products...\n",
      "   Found 184 unique brands.\n",
      "   Top 5 Giants: [('solimo', 4348), ('amazon basics', 506), ('stone & beam', 213), ('rivet', 152), ('amazon collection', 149)]\n",
      "\n",
      "‚úÖ Brand Analysis Complete. Saved to data/brand_popularity.json\n"
     ]
    }
   ],
   "source": [
    "!python3 brand_analyzer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6314d038-2a7a-4470-8486-a71e80a26dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Applying Causal Filter (Derived Rank from Visibility)...\n",
      "\n",
      "--- REPORT ---\n",
      "Total Queries: 10\n",
      "Generated Pairs: 204\n",
      "Saved to data/causal_pairs.json\n"
     ]
    }
   ],
   "source": [
    "!python3 causal_filter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "929ef5c6-adcd-4837-b556-4d0167094f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/12/11 - 16:08:48 |\u001b[97;42m 200 \u001b[0m| 10.803674254s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: To maximize relevance and SEO performance, always translate the visual reality into the text with specific, image‚Äëcentric language‚Äîdetail pattern shape, precise color combinations, and distinctive finishes‚Äîrather than relying on generic descriptors.\n",
      "      üíæ Saved progress (163 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:08:58 |\u001b[97;42m 200 \u001b[0m| 10.236781074s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: For higher ranking, ensure that the description covers all salient visual attributes‚Äîshape, pattern, texture, and accurate colors‚Äîand that it is aligned with the actual product type. Avoid generic or misleading terms that leave out critical visual details.\n",
      "      üíæ Saved progress (164 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:09:07 |\u001b[97;42m 200 \u001b[0m|  9.052318635s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: In SEO content for visual products, always translate the image into precise visual language‚Äîname the exact color, material, and texture that a viewer can see‚Äîrather than using generic or technical terms. This specificity bridges the translation gap and improves relevance to the query.\n",
      "      üíæ Saved progress (165 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:09:15 |\u001b[97;42m 200 \u001b[0m|   7.93164366s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When describing a product‚Äôs visual attributes, always use specific, observable terms (e.g., ‚Äòvelvet‚Äô, ‚Äòsubtle grooves‚Äô, ‚Äòbright blue‚Äô, ‚Äòwhite‚Äëand‚Äëblue stripes‚Äô) rather than vague or inaccurate descriptors. Accurate color names and texture references align the text with the image, strengthening relevance and improving SEO ranking.\n",
      "      üíæ Saved progress (166 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:09:25 |\u001b[97;42m 200 \u001b[0m| 10.320374468s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: Always describe the specific texture, pattern, and color that are visible in the image. Avoid generic material terms or mislabeling features‚Äîprecise visual details create stronger relevance and improve ranking.\n",
      "      üíæ Saved progress (167 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:09:33 |\u001b[97;42m 200 \u001b[0m|  7.628803229s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: Ensure every description fully captures the visible features of the product‚Äîcolor, texture, pattern, material, and shape‚Äîso the text faithfully reflects the image. Missing or irrelevant details create a completeness gap that harms relevance and SEO.\n",
      "      üíæ Saved progress (168 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:09:43 |\u001b[97;42m 200 \u001b[0m| 10.403999344s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: Use specific, sensory‚Äërich descriptors that reflect the exact texture, pattern, and color seen in the image. Avoid generic or broad terms; instead, name the material, texture detail, and precise color shade to create a vivid, accurate visual narrative.\n",
      "      üíæ Saved progress (169 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:09:52 |\u001b[97;42m 200 \u001b[0m|  8.648138911s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When writing product copy, always ensure that every visible visual feature‚Äîcolor, texture, pattern, and style‚Äîis accurately and completely described. Missing or incorrect details create a translation gap that harms relevance and ranking.\n",
      "      üíæ Saved progress (170 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:10:06 |\u001b[97;42m 200 \u001b[0m|  13.86991439s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When translating visual data into text, ensure the description is **complete**: capture all visible attributes‚Äîcolor, pattern, arrangement, texture‚Äîand reference them explicitly. Even if the product lacks complex designs, note its absence to align the textual narrative with the visual reality.\n",
      "      üíæ Saved progress (171 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:10:23 |\u001b[97;42m 200 \u001b[0m| 16.954648654s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: To outperform competitors, product descriptions should blend concrete visual attributes with evocative, lifestyle‚Äëdriven language that communicates the *feel* and *style* of the item, not just its specs.\n",
      "      üíæ Saved progress (172 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:10:37 |\u001b[97;42m 200 \u001b[0m| 13.954670921s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When translating visual data to text, ensure the description covers all primary visual attributes of the product type‚Äîshape, material, color, pattern, and functional details. Omit unrelated objects or product categories, as this can lead to incomplete, misleading, or low‚Äëranking content.\n",
      "      üíæ Saved progress (173 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:10:56 |\u001b[97;42m 200 \u001b[0m| 19.417356088s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When translating visual data into text, aim for a comprehensive description that captures not only the visible pattern or color but also material quality, functional features, and overall style.  A full‚Äëfeature narrative increases relevance and attractiveness, helping listings rank higher than those that list only a narrow visual detail.\n",
      "      üíæ Saved progress (174 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:11:05 |\u001b[97;42m 200 \u001b[0m|  8.708916923s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When converting visual data into textual descriptions, always ensure that the **core item type** and its **functional attributes** are captured. Completeness‚Äîmentioning what the item is (e.g., pillow cover), how it is used, and its key visual details‚Äîdrastically improves relevance and SEO performance. Avoid describing the scene in generic terms or shifting focus to unrelated objects; keep the description tightly aligned with the main visual element.\n",
      "      üíæ Saved progress (175 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:11:14 |\u001b[97;42m 200 \u001b[0m|  8.737137107s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When translating visual data into text, prioritize specific, observable attributes‚Äîsuch as exact texture descriptions, precise color tones, and pattern orientation‚Äîrather than broad or generic terms. This clarity helps both users and search engines accurately match the product to the visual intent of the query.\n",
      "      üíæ Saved progress (176 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:11:22 |\u001b[97;42m 200 \u001b[0m|  8.614709308s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When describing visual attributes, always use the most specific, image‚Äëaligned terminology (e.g., ‚Äòyarn‚Äëdyed diagonal stripes‚Äô vs. ‚Äòplain white fabric‚Äô). Accurate, detailed visual language directly supports search relevance and improves ranking.\n",
      "      üíæ Saved progress (177 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:11:28 |\u001b[97;42m 200 \u001b[0m|  5.688128443s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When writing product descriptions, always align the visual narrative with the actual product type and include every visible feature that helps the shopper recognize and imagine the item. Descriptions should capture shape, material, pattern, color, and any functional details‚Äîmissing or irrelevant elements create a completeness gap and hurt relevance.\n",
      "      üíæ Saved progress (178 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:11:37 |\u001b[97;42m 200 \u001b[0m|  8.715007359s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: Always ground your visual description in the specific patterns, colors, shapes, and textures that the image actually displays. Use concrete, perceptible terms (e.g., ‚Äòdiagonal, yarn‚Äëdyed stripes‚Äô or ‚Äògray‚Äëand‚Äëwhite checkered pattern‚Äô) rather than broad descriptors like ‚Äòplaid‚Äô or ‚Äòneutral shades‚Äô to create a clear, accurate visual bridge for search relevance.\n",
      "      üíæ Saved progress (179 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:11:45 |\u001b[97;42m 200 \u001b[0m|  8.420578746s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When translating visual data to text, always anchor the description in the specific visual details observed: use exact colors, pattern orientations, and textures. Avoid generic or mismatched terminology; accurate specificity aligns the textual description with the visual reality and improves relevance and ranking.\n",
      "      üíæ Saved progress (180 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:11:58 |\u001b[97;42m 200 \u001b[0m| 12.722728831s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When describing a product‚Äôs visual reality, always list all prominent surface patterns and features (e.g., tufting, stitching, knots, ties) that a viewer can see. Avoid relying solely on generic terms like ‚Äòstripe‚Äô or ‚Äòcolor‚Äô‚Äîinclude the specific texture or pattern that distinguishes the item.\n",
      "      üíæ Saved progress (181 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:12:07 |\u001b[97;42m 200 \u001b[0m|  9.244782721s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: To translate visual reality into effective SEO copy, list all visible decorative elements (e.g., tufting, hardware, trim) and material textures, not just color or a single aesthetic feature. This completeness ensures the text mirrors the full visual experience.\n",
      "      üíæ Saved progress (182 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:12:16 |\u001b[97;42m 200 \u001b[0m|  8.770643459s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: Always include the most salient visual traits that a user will see ‚Äì primary color, material texture, and shape ‚Äì in the product description. Omitting these core attributes creates a completeness gap that can hurt relevance and ranking.\n",
      "      üíæ Saved progress (183 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:12:30 |\u001b[97;42m 200 \u001b[0m| 13.767845036s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When describing visually rich products, use precise, texture‚Äëoriented terms that mirror the actual appearance (e.g., tufting, weave, pattern geometry, color depth). Avoid generic descriptors that leave visual context vague; the more the copy mirrors what the eye sees, the stronger the SEO signal and the higher the ranking.\n",
      "      üíæ Saved progress (184 rules total)\n",
      "   prompting LLM for Geometric print thro...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:12:45 |\u001b[97;42m 200 \u001b[0m| 14.916305928s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When describing visual products, match the text to the precise visual details in the image‚Äîspecific textures, patterns, or structural features‚Äîrather than relying on generic or abstract language. This specificity bridges the perception gap and boosts relevance in search.\n",
      "      üíæ Saved progress (185 rules total)\n",
      "\n",
      "üìÇ Processing Query Group 8/8: 'Ceramic vase modern white'\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:12:50 |\u001b[97;42m 200 \u001b[0m|  5.033276183s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When describing visual attributes, prioritize specific, perceptible details‚Äîsuch as exact color tones, textures, or patterns‚Äîover generic terms. This precision captures the unique visual identity of the product and aligns the textual description more closely with the image.\n",
      "      üíæ Saved progress (186 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:12:57 |\u001b[97;42m 200 \u001b[0m|  7.124702836s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When describing a product‚Äôs visual attributes, ensure every prominent feature‚Äîcolor, pattern, shape, and any unique finishing technique‚Äîis captured. Omitting or misrepresenting these elements leaves a gap that can lower relevance in search ranking.\n",
      "      üíæ Saved progress (187 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:13:09 |\u001b[97;42m 200 \u001b[0m|  11.71426422s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When optimizing product listings, always include every visible visual feature‚Äîcolor patterns, textures, and design effects‚Äîin the description. Comprehensive, specific visuals help search algorithms match queries and drive higher rankings.\n",
      "      üíæ Saved progress (188 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:13:17 |\u001b[97;42m 200 \u001b[0m|  8.052542181s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When translating visual data into text, always include every prominent visual element‚Äîcolor schemes, patterns, shapes, and textures. Avoid generic terms alone; instead, describe the specific visual details that the user can see, ensuring the textual description fully captures the product‚Äôs appearance.\n",
      "      üíæ Saved progress (189 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:13:24 |\u001b[97;42m 200 \u001b[0m|  7.279469057s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: Always translate the actual visual details‚Äîcolor gradients, patterns, textures, and unique design elements‚Äîinto specific, vivid language in the description. Avoid generic material or color terms that do not capture what the viewer actually sees.\n",
      "      üíæ Saved progress (190 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:13:32 |\u001b[97;42m 200 \u001b[0m|  8.315743955s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When translating visual data into text, ensure every observable feature‚Äîcolor scheme, texture, pattern, and material‚Äîis captured. Use specific descriptors (e.g., \"hand‚Äëdipped reactive glazing\" or \"melting drip effect\") rather than generic terms, so the description fully reflects what the viewer sees.\n",
      "      üíæ Saved progress (191 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:13:44 |\u001b[97;42m 200 \u001b[0m| 11.502557012s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When describing visual attributes for search relevance, prioritize concrete, image‚Äëgrounded details‚Äîcolor gradients, textures, patterns, and visual effects‚Äîover generic material categories or functional specs. Specificity that mirrors what is actually seen in the image strengthens the semantic match with user intent and improves ranking.\n",
      "      üíæ Saved progress (192 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:13:59 |\u001b[97;42m 200 \u001b[0m| 15.342392349s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: For visual queries, translate every observable attribute into specific, vivid text‚Äîdescribe exact colors, shapes, patterns, and design effects. Avoid generic material or durability terms that do not reflect what the user sees.\n",
      "      üíæ Saved progress (193 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:14:07 |\u001b[97;42m 200 \u001b[0m|  8.030060316s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When translating visual data to text, ensure all observable structural components‚Äîsuch as stands, bases, or distinctive surface textures‚Äîare explicitly described. Omitting such elements results in an incomplete portrayal that can hurt ranking.\n",
      "      üíæ Saved progress (194 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:14:14 |\u001b[97;42m 200 \u001b[0m|  6.582438337s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When describing a product, ensure that every prominent visual component‚Äîmaterial, color, shape, and any contrasting elements‚Äîis explicitly mentioned. Omitting a major feature (e.g., a wood base) creates a completeness gap that harms relevance and SEO performance.\n",
      "      üíæ Saved progress (195 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:14:25 |\u001b[97;42m 200 \u001b[0m| 11.299798697s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When translating visual data to text, use specific material names, color nuances, and finish details that mirror what is actually seen. Generic terms dilute the relevance and hinder SEO performance.\n",
      "      üíæ Saved progress (196 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:14:35 |\u001b[97;42m 200 \u001b[0m| 10.309365996s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When optimizing for visual relevance, describe each visible material, finish, and architectural element with specific terminology that mirrors what is actually seen in the image. Avoid generic class names; instead, include concrete descriptors such as color, texture, and finish details to bridge the translation gap.\n",
      "      üíæ Saved progress (197 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:14:41 |\u001b[97;42m 200 \u001b[0m|  6.072921127s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When translating visual data into text, ensure all visible components (e.g., base, stand, material finish) are captured; missing any element reduces completeness and can hurt ranking.\n",
      "      üíæ Saved progress (198 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:14:51 |\u001b[97;42m 200 \u001b[0m|  9.257314403s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: When translating visual data to text, always describe every prominent visual feature‚Äîshape, material, color, finish, dimensions, and supporting elements. Ensure the product type matches what is actually visible, and avoid switching to a different item or leaving out critical attributes. This completeness aligns the description with the viewer‚Äôs expectations and boosts relevance.\n",
      "      üíæ Saved progress (199 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:15:01 |\u001b[97;42m 200 \u001b[0m| 10.198467043s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: Accurately capture the primary visual components and product type present in the image; omitting or misrepresenting core features causes the description to fail the relevance check and drop in ranking.\n",
      "      üíæ Saved progress (200 rules total)\n",
      "   prompting LLM for Ceramic vase modern ...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:15:08 |\u001b[97;42m 200 \u001b[0m|  7.500652708s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "   üí° New Rule: Always ensure the textual description is a complete, faithful translation of the visible product. Include every prominent visual attribute (shape, material, color, context) that a viewer can see. Avoid describing unrelated items or generic style cues that do not reflect the actual object in the image.\n",
      "      üíæ Saved progress (201 rules total)\n",
      "\n",
      "‚úÖ Exploration Complete. Total Unique Rules: 201\n",
      "   Final save to data/optimization_rules.json\n"
     ]
    }
   ],
   "source": [
    "!python3 explainer_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a4b9175-033f-499d-8edd-dca8dbcd69c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Adaptive Aggregator (Model: all-MiniLM-L6-v2)...\n",
      "   Vectorizing 201 rules...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00,  7.63it/s]\n",
      "   Auto-detecting semantic structures (Affinity Propagation)...\n",
      "   üîé Found 21 natural semantic clusters.\n",
      "\n",
      "üß† Synthesizing Principles from 21 clusters...\n",
      "      Processing Cluster 0 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "Connection error while contacting Ollama: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71bbbaa60350>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Ollama server seems down. Attempting to start it...\n",
      "time=2025-12-11T16:17:24.147+05:30 level=INFO source=routes.go:1544 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:4 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/ajit_2411ai19/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2025-12-11T16:17:24.148+05:30 level=INFO source=images.go:522 msg=\"total blobs: 5\"\n",
      "time=2025-12-11T16:17:24.148+05:30 level=INFO source=images.go:529 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-12-11T16:17:24.148+05:30 level=INFO source=routes.go:1597 msg=\"Listening on 127.0.0.1:11434 (version 0.12.11)\"\n",
      "time=2025-12-11T16:17:24.149+05:30 level=INFO source=runner.go:67 msg=\"discovering available GPUs...\"\n",
      "time=2025-12-11T16:17:24.149+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 45701\"\n",
      "time=2025-12-11T16:17:24.476+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 35479\"\n",
      "time=2025-12-11T16:17:24.631+05:30 level=INFO source=types.go:42 msg=\"inference compute\" id=GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 filter_id=\"\" library=CUDA compute=8.0 name=CUDA0 description=\"NVIDIA A100-PCIE-40GB\" libdirs=ollama,cuda_v12 driver=12.2 pci_id=0000:e1:00.0 type=discrete total=\"40.0 GiB\" available=\"38.6 GiB\"\n",
      "Attempt 2/8 to call Ollama...\n",
      "time=2025-12-11T16:17:34.561+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 40755\"\n",
      "time=2025-12-11T16:17:35.165+05:30 level=INFO source=server.go:209 msg=\"enabling flash attention\"\n",
      "time=2025-12-11T16:17:35.165+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --model /home/ajit_2411ai19/.ollama/models/blobs/sha256-e7b273f9636059a689e3ddcab3716e4f65abe0143ac978e46673ad0e52d09efb --port 34273\"\n",
      "time=2025-12-11T16:17:35.166+05:30 level=INFO source=sched.go:443 msg=\"system memory\" total=\"251.7 GiB\" free=\"216.0 GiB\" free_swap=\"9.3 GiB\"\n",
      "time=2025-12-11T16:17:35.166+05:30 level=INFO source=sched.go:450 msg=\"gpu memory\" id=GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 library=CUDA available=\"38.2 GiB\" free=\"38.6 GiB\" minimum=\"457.0 MiB\" overhead=\"0 B\"\n",
      "time=2025-12-11T16:17:35.166+05:30 level=INFO source=server.go:702 msg=\"loading model\" \"model layers\"=25 requested=-1\n",
      "time=2025-12-11T16:17:35.194+05:30 level=INFO source=runner.go:1398 msg=\"starting ollama engine\"\n",
      "time=2025-12-11T16:17:35.195+05:30 level=INFO source=runner.go:1433 msg=\"Server listening on 127.0.0.1:34273\"\n",
      "time=2025-12-11T16:17:35.200+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T16:17:35.319+05:30 level=INFO source=ggml.go:136 msg=\"\" architecture=gptoss file_type=MXFP4 name=\"\" description=\"\" num_tensors=459 num_key_values=32\n",
      "load_backend: loaded CPU backend from /home/ajit_2411ai19/opt/ollama/lib/ollama/libggml-cpu-haswell.so\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100-PCIE-40GB, compute capability 8.0, VMM: yes, ID: GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14\n",
      "load_backend: loaded CUDA backend from /home/ajit_2411ai19/opt/ollama/lib/ollama/cuda_v12/libggml-cuda.so\n",
      "time=2025-12-11T16:17:35.520+05:30 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-12-11T16:17:36.302+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T16:17:36.533+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T16:17:36.533+05:30 level=INFO source=ggml.go:482 msg=\"offloading 24 repeating layers to GPU\"\n",
      "time=2025-12-11T16:17:36.533+05:30 level=INFO source=ggml.go:489 msg=\"offloading output layer to GPU\"\n",
      "time=2025-12-11T16:17:36.533+05:30 level=INFO source=ggml.go:494 msg=\"offloaded 25/25 layers to GPU\"\n",
      "time=2025-12-11T16:17:36.534+05:30 level=INFO source=device.go:240 msg=\"model weights\" device=CUDA0 size=\"11.8 GiB\"\n",
      "time=2025-12-11T16:17:36.534+05:30 level=INFO source=device.go:245 msg=\"model weights\" device=CPU size=\"1.1 GiB\"\n",
      "time=2025-12-11T16:17:36.534+05:30 level=INFO source=device.go:251 msg=\"kv cache\" device=CUDA0 size=\"300.0 MiB\"\n",
      "time=2025-12-11T16:17:36.534+05:30 level=INFO source=device.go:262 msg=\"compute graph\" device=CUDA0 size=\"178.6 MiB\"\n",
      "time=2025-12-11T16:17:36.534+05:30 level=INFO source=device.go:267 msg=\"compute graph\" device=CPU size=\"5.6 MiB\"\n",
      "time=2025-12-11T16:17:36.534+05:30 level=INFO source=device.go:272 msg=\"total memory\" size=\"13.3 GiB\"\n",
      "time=2025-12-11T16:17:36.534+05:30 level=INFO source=sched.go:517 msg=\"loaded runners\" count=1\n",
      "time=2025-12-11T16:17:36.534+05:30 level=INFO source=server.go:1294 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-12-11T16:17:36.534+05:30 level=INFO source=server.go:1328 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "time=2025-12-11T16:17:39.797+05:30 level=INFO source=server.go:1332 msg=\"llama runner started in 4.63 seconds\"\n",
      "[GIN] 2025/12/11 - 16:17:43 |\u001b[97;42m 200 \u001b[0m|  9.113646336s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 0 Batch 2...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:17:46 |\u001b[97;42m 200 \u001b[0m|    3.0136631s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:17:52 |\u001b[97;42m 200 \u001b[0m|  6.254016021s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 1 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:17:56 |\u001b[97;42m 200 \u001b[0m|  4.323277222s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:02 |\u001b[97;42m 200 \u001b[0m|  5.321843111s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 2 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:06 |\u001b[97;42m 200 \u001b[0m|  4.802617295s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:10 |\u001b[97;42m 200 \u001b[0m|  3.466856093s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 3 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:14 |\u001b[97;42m 200 \u001b[0m|  3.646790785s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:18 |\u001b[97;42m 200 \u001b[0m|  4.757372905s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 4 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:23 |\u001b[97;42m 200 \u001b[0m|  4.158737521s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:29 |\u001b[97;42m 200 \u001b[0m|  6.689577233s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 5 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:33 |\u001b[97;42m 200 \u001b[0m|  3.874119625s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:37 |\u001b[97;42m 200 \u001b[0m|   3.95042841s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 6 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:42 |\u001b[97;42m 200 \u001b[0m|  4.674609875s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:44 |\u001b[97;42m 200 \u001b[0m|  2.452487942s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 7 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:50 |\u001b[97;42m 200 \u001b[0m|  5.977074842s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:53 |\u001b[97;42m 200 \u001b[0m|  2.681823812s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 8 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:18:57 |\u001b[97;42m 200 \u001b[0m|  4.030202607s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:01 |\u001b[97;42m 200 \u001b[0m|  3.923282211s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 9 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:09 |\u001b[97;42m 200 \u001b[0m|  7.981656766s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:13 |\u001b[97;42m 200 \u001b[0m|   4.49596376s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 10 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:18 |\u001b[97;42m 200 \u001b[0m|  4.271624433s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:22 |\u001b[97;42m 200 \u001b[0m|  4.526289313s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 11 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:25 |\u001b[97;42m 200 \u001b[0m|  3.197754034s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:29 |\u001b[97;42m 200 \u001b[0m|  3.856307686s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 12 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:33 |\u001b[97;42m 200 \u001b[0m|  3.803952268s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:36 |\u001b[97;42m 200 \u001b[0m|  2.614435687s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 13 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:39 |\u001b[97;42m 200 \u001b[0m|  3.386639622s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:42 |\u001b[97;42m 200 \u001b[0m|  3.375992063s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 14 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:46 |\u001b[97;42m 200 \u001b[0m|  3.801422649s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 14 Batch 2...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:52 |\u001b[97;42m 200 \u001b[0m|  5.485818248s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:54 |\u001b[97;42m 200 \u001b[0m|  2.733938766s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 15 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:19:59 |\u001b[97;42m 200 \u001b[0m|  4.457776787s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:20:06 |\u001b[97;42m 200 \u001b[0m|  6.801373965s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 16 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:20:14 |\u001b[97;42m 200 \u001b[0m|  7.933756816s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:20:17 |\u001b[97;42m 200 \u001b[0m|  3.918987831s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 17 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:20:22 |\u001b[97;42m 200 \u001b[0m|  4.132230849s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 17 Batch 2...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:20:26 |\u001b[97;42m 200 \u001b[0m|  4.377752879s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:20:30 |\u001b[97;42m 200 \u001b[0m|  3.863462331s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 18 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:20:34 |\u001b[97;42m 200 \u001b[0m|  4.294785082s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:20:39 |\u001b[97;42m 200 \u001b[0m|  4.884872123s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 19 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:20:43 |\u001b[97;42m 200 \u001b[0m|  3.696218309s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:20:48 |\u001b[97;42m 200 \u001b[0m|  4.863166117s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "      Processing Cluster 20 Batch 1...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:20:52 |\u001b[97;42m 200 \u001b[0m|  4.461416472s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "[GIN] 2025/12/11 - 16:20:57 |\u001b[97;42m 200 \u001b[0m|  4.664753217s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "\n",
      "‚úÖ Aggregation Complete.\n",
      "   Generated 21 Principles from 201 observations.\n",
      "   Saved to data/mgeo_principles.json\n"
     ]
    }
   ],
   "source": [
    "!python3 rule_aggregator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb81b6ef-0403-4495-aaed-b16f03437be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Refining 21 principles into an Orthogonal Set...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "Connection error while contacting Ollama: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x790706178890>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Ollama server seems down. Attempting to start it...\n",
      "time=2025-12-11T16:23:27.424+05:30 level=INFO source=routes.go:1544 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:4 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/ajit_2411ai19/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2025-12-11T16:23:27.427+05:30 level=INFO source=images.go:522 msg=\"total blobs: 5\"\n",
      "time=2025-12-11T16:23:27.427+05:30 level=INFO source=images.go:529 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-12-11T16:23:27.428+05:30 level=INFO source=routes.go:1597 msg=\"Listening on 127.0.0.1:11434 (version 0.12.11)\"\n",
      "time=2025-12-11T16:23:27.428+05:30 level=INFO source=runner.go:67 msg=\"discovering available GPUs...\"\n",
      "time=2025-12-11T16:23:27.429+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 34511\"\n",
      "time=2025-12-11T16:23:27.744+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 38263\"\n",
      "time=2025-12-11T16:23:27.891+05:30 level=INFO source=types.go:42 msg=\"inference compute\" id=GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 filter_id=\"\" library=CUDA compute=8.0 name=CUDA0 description=\"NVIDIA A100-PCIE-40GB\" libdirs=ollama,cuda_v12 driver=12.2 pci_id=0000:e1:00.0 type=discrete total=\"40.0 GiB\" available=\"39.4 GiB\"\n",
      "Attempt 2/8 to call Ollama...\n",
      "time=2025-12-11T16:23:37.835+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 37635\"\n",
      "time=2025-12-11T16:23:38.483+05:30 level=INFO source=server.go:209 msg=\"enabling flash attention\"\n",
      "time=2025-12-11T16:23:38.483+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --model /home/ajit_2411ai19/.ollama/models/blobs/sha256-e7b273f9636059a689e3ddcab3716e4f65abe0143ac978e46673ad0e52d09efb --port 33539\"\n",
      "time=2025-12-11T16:23:38.484+05:30 level=INFO source=sched.go:443 msg=\"system memory\" total=\"251.7 GiB\" free=\"219.0 GiB\" free_swap=\"9.3 GiB\"\n",
      "time=2025-12-11T16:23:38.485+05:30 level=INFO source=sched.go:450 msg=\"gpu memory\" id=GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 library=CUDA available=\"38.9 GiB\" free=\"39.4 GiB\" minimum=\"457.0 MiB\" overhead=\"0 B\"\n",
      "time=2025-12-11T16:23:38.485+05:30 level=INFO source=server.go:702 msg=\"loading model\" \"model layers\"=25 requested=-1\n",
      "time=2025-12-11T16:23:38.516+05:30 level=INFO source=runner.go:1398 msg=\"starting ollama engine\"\n",
      "time=2025-12-11T16:23:38.516+05:30 level=INFO source=runner.go:1433 msg=\"Server listening on 127.0.0.1:33539\"\n",
      "time=2025-12-11T16:23:38.519+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T16:23:38.636+05:30 level=INFO source=ggml.go:136 msg=\"\" architecture=gptoss file_type=MXFP4 name=\"\" description=\"\" num_tensors=459 num_key_values=32\n",
      "load_backend: loaded CPU backend from /home/ajit_2411ai19/opt/ollama/lib/ollama/libggml-cpu-haswell.so\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100-PCIE-40GB, compute capability 8.0, VMM: yes, ID: GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14\n",
      "load_backend: loaded CUDA backend from /home/ajit_2411ai19/opt/ollama/lib/ollama/cuda_v12/libggml-cuda.so\n",
      "time=2025-12-11T16:23:38.839+05:30 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-12-11T16:23:39.571+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T16:23:39.802+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T16:23:39.802+05:30 level=INFO source=ggml.go:482 msg=\"offloading 24 repeating layers to GPU\"\n",
      "time=2025-12-11T16:23:39.802+05:30 level=INFO source=ggml.go:489 msg=\"offloading output layer to GPU\"\n",
      "time=2025-12-11T16:23:39.802+05:30 level=INFO source=ggml.go:494 msg=\"offloaded 25/25 layers to GPU\"\n",
      "time=2025-12-11T16:23:39.803+05:30 level=INFO source=device.go:240 msg=\"model weights\" device=CUDA0 size=\"11.8 GiB\"\n",
      "time=2025-12-11T16:23:39.803+05:30 level=INFO source=device.go:245 msg=\"model weights\" device=CPU size=\"1.1 GiB\"\n",
      "time=2025-12-11T16:23:39.803+05:30 level=INFO source=device.go:251 msg=\"kv cache\" device=CUDA0 size=\"300.0 MiB\"\n",
      "time=2025-12-11T16:23:39.803+05:30 level=INFO source=device.go:262 msg=\"compute graph\" device=CUDA0 size=\"178.6 MiB\"\n",
      "time=2025-12-11T16:23:39.803+05:30 level=INFO source=device.go:267 msg=\"compute graph\" device=CPU size=\"5.6 MiB\"\n",
      "time=2025-12-11T16:23:39.803+05:30 level=INFO source=device.go:272 msg=\"total memory\" size=\"13.3 GiB\"\n",
      "time=2025-12-11T16:23:39.803+05:30 level=INFO source=sched.go:517 msg=\"loaded runners\" count=1\n",
      "time=2025-12-11T16:23:39.803+05:30 level=INFO source=server.go:1294 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-12-11T16:23:39.803+05:30 level=INFO source=server.go:1328 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "time=2025-12-11T16:23:43.065+05:30 level=INFO source=server.go:1332 msg=\"llama runner started in 4.58 seconds\"\n",
      "[GIN] 2025/12/11 - 16:23:56 |\u001b[97;42m 200 \u001b[0m| 19.486379076s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "‚úÖ Refinement Complete.\n",
      "   Collapsed 21 redundant rules -> 4 orthogonal strategies.\n",
      "   Saved to data/mgeo_principles_refined.json\n"
     ]
    }
   ],
   "source": [
    "!python3 rule_refiner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa8619-7474-4987-953c-40fef2f4bbdd",
   "metadata": {},
   "source": [
    "## Phase II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa485955-4c34-44cf-8f9f-dc31beed409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Identifying High-Potential Targets...\n",
      "‚úÖ Selection Complete.\n",
      "   Identified targets for 8 queries.\n",
      "   Saved to data/target_candidates.json\n",
      "\n",
      "Example Top Candidate for 'Women's floral embroidery slip-on shoes':\n",
      "   ID: B071JM2QL7 (Rank 10 | Vis 0.0)\n",
      "   Score: 12.16\n",
      "   Diagnosis: The Winner‚Äôs text highlights the actual suede texture, the slip‚Äëon form and the pointed toe shape that are visible in the image, while also noting the leather lining ‚Äì capturing every prominent visual\n"
     ]
    }
   ],
   "source": [
    "!python3 target_source_selector.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a499f9aa-fbdc-49d4-b698-b96fb329e67b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading Phase 1 Data...\n",
      "\n",
      "üéØ TARGET SELECTED:\n",
      "   Query: Women's floral embroidery slip-on shoes\n",
      "   Item ID: B071JM2QL7\n",
      "   Current Visibility: 0.0\n",
      "   ‚úçÔ∏è Optimizer applying principles to 'B071JM2QL7'...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "Connection error while contacting Ollama: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7715a5da92b0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Ollama server seems down. Attempting to start it...\n",
      "time=2025-12-11T17:44:12.059+05:30 level=INFO source=routes.go:1544 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:4 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/ajit_2411ai19/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2025-12-11T17:44:12.061+05:30 level=INFO source=images.go:522 msg=\"total blobs: 5\"\n",
      "time=2025-12-11T17:44:12.062+05:30 level=INFO source=images.go:529 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-12-11T17:44:12.062+05:30 level=INFO source=routes.go:1597 msg=\"Listening on 127.0.0.1:11434 (version 0.12.11)\"\n",
      "time=2025-12-11T17:44:12.064+05:30 level=INFO source=runner.go:67 msg=\"discovering available GPUs...\"\n",
      "time=2025-12-11T17:44:12.065+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 42939\"\n",
      "time=2025-12-11T17:44:12.425+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 44075\"\n",
      "time=2025-12-11T17:44:12.576+05:30 level=INFO source=types.go:42 msg=\"inference compute\" id=GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 filter_id=\"\" library=CUDA compute=8.0 name=CUDA0 description=\"NVIDIA A100-PCIE-40GB\" libdirs=ollama,cuda_v12 driver=12.2 pci_id=0000:e1:00.0 type=discrete total=\"40.0 GiB\" available=\"39.4 GiB\"\n",
      "Attempt 2/8 to call Ollama...\n",
      "time=2025-12-11T17:44:22.476+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 36687\"\n",
      "time=2025-12-11T17:44:23.103+05:30 level=INFO source=server.go:209 msg=\"enabling flash attention\"\n",
      "time=2025-12-11T17:44:23.103+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --model /home/ajit_2411ai19/.ollama/models/blobs/sha256-e7b273f9636059a689e3ddcab3716e4f65abe0143ac978e46673ad0e52d09efb --port 39413\"\n",
      "time=2025-12-11T17:44:23.105+05:30 level=INFO source=sched.go:443 msg=\"system memory\" total=\"251.7 GiB\" free=\"222.7 GiB\" free_swap=\"9.3 GiB\"\n",
      "time=2025-12-11T17:44:23.105+05:30 level=INFO source=sched.go:450 msg=\"gpu memory\" id=GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 library=CUDA available=\"38.9 GiB\" free=\"39.4 GiB\" minimum=\"457.0 MiB\" overhead=\"0 B\"\n",
      "time=2025-12-11T17:44:23.105+05:30 level=INFO source=server.go:702 msg=\"loading model\" \"model layers\"=25 requested=-1\n",
      "time=2025-12-11T17:44:23.134+05:30 level=INFO source=runner.go:1398 msg=\"starting ollama engine\"\n",
      "time=2025-12-11T17:44:23.135+05:30 level=INFO source=runner.go:1433 msg=\"Server listening on 127.0.0.1:39413\"\n",
      "time=2025-12-11T17:44:23.139+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T17:44:23.267+05:30 level=INFO source=ggml.go:136 msg=\"\" architecture=gptoss file_type=MXFP4 name=\"\" description=\"\" num_tensors=459 num_key_values=32\n",
      "load_backend: loaded CPU backend from /home/ajit_2411ai19/opt/ollama/lib/ollama/libggml-cpu-haswell.so\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100-PCIE-40GB, compute capability 8.0, VMM: yes, ID: GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14\n",
      "load_backend: loaded CUDA backend from /home/ajit_2411ai19/opt/ollama/lib/ollama/cuda_v12/libggml-cuda.so\n",
      "time=2025-12-11T17:44:23.487+05:30 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-12-11T17:44:24.282+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T17:44:24.520+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T17:44:24.521+05:30 level=INFO source=ggml.go:482 msg=\"offloading 24 repeating layers to GPU\"\n",
      "time=2025-12-11T17:44:24.521+05:30 level=INFO source=ggml.go:489 msg=\"offloading output layer to GPU\"\n",
      "time=2025-12-11T17:44:24.521+05:30 level=INFO source=ggml.go:494 msg=\"offloaded 25/25 layers to GPU\"\n",
      "time=2025-12-11T17:44:24.521+05:30 level=INFO source=device.go:240 msg=\"model weights\" device=CUDA0 size=\"11.8 GiB\"\n",
      "time=2025-12-11T17:44:24.521+05:30 level=INFO source=device.go:245 msg=\"model weights\" device=CPU size=\"1.1 GiB\"\n",
      "time=2025-12-11T17:44:24.521+05:30 level=INFO source=device.go:251 msg=\"kv cache\" device=CUDA0 size=\"300.0 MiB\"\n",
      "time=2025-12-11T17:44:24.521+05:30 level=INFO source=device.go:262 msg=\"compute graph\" device=CUDA0 size=\"178.6 MiB\"\n",
      "time=2025-12-11T17:44:24.521+05:30 level=INFO source=device.go:267 msg=\"compute graph\" device=CPU size=\"5.6 MiB\"\n",
      "time=2025-12-11T17:44:24.521+05:30 level=INFO source=device.go:272 msg=\"total memory\" size=\"13.3 GiB\"\n",
      "time=2025-12-11T17:44:24.521+05:30 level=INFO source=sched.go:517 msg=\"loaded runners\" count=1\n",
      "time=2025-12-11T17:44:24.521+05:30 level=INFO source=server.go:1294 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-12-11T17:44:24.522+05:30 level=INFO source=server.go:1328 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "time=2025-12-11T17:44:28.036+05:30 level=INFO source=server.go:1332 msg=\"llama runner started in 4.93 seconds\"\n",
      "[GIN] 2025/12/11 - 17:44:39 |\u001b[97;42m 200 \u001b[0m| 17.226174797s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "\n",
      "‚ú® OPTIMIZATION SUCCESSFUL!\n",
      "   Old Title: Red Wagon Girls' Slip on Skate Low-Top Sneakers, B...\n",
      "   New Title: Women's White Leather Slip‚ÄëOn Shoes with Pink Sole...\n",
      "   üíæ Saved optimized product to data/optimized_product.json\n",
      "   Ready for Verification.\n"
     ]
    }
   ],
   "source": [
    "!python3 run_optimizer.py --q_idx 0 --c_idx 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "803cfe37-ec3c-49ff-9cbf-7e31c68799bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Verifying Optimization for 'B071JM2QL7'...\n",
      "   Query: Women's floral embroidery slip-on shoes\n",
      "   Baseline: Rank 10 | Vis 0.0\n",
      "   üîÑ Swapping in Optimized Content...\n",
      "   ü§ñ Running Simulator (Generation Step)...\n",
      "Attempting to generate response with Ollama...\n",
      "Attempt 1/8 to call Ollama...\n",
      "Connection error while contacting Ollama: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c42d2cb26c0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Ollama server seems down. Attempting to start it...\n",
      "time=2025-12-11T18:09:31.002+05:30 level=INFO source=routes.go:1544 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:4 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/ajit_2411ai19/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2025-12-11T18:09:31.003+05:30 level=INFO source=images.go:522 msg=\"total blobs: 5\"\n",
      "time=2025-12-11T18:09:31.003+05:30 level=INFO source=images.go:529 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-12-11T18:09:31.005+05:30 level=INFO source=routes.go:1597 msg=\"Listening on 127.0.0.1:11434 (version 0.12.11)\"\n",
      "time=2025-12-11T18:09:31.007+05:30 level=INFO source=runner.go:67 msg=\"discovering available GPUs...\"\n",
      "time=2025-12-11T18:09:31.008+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 42867\"\n",
      "time=2025-12-11T18:09:31.335+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 46533\"\n",
      "time=2025-12-11T18:09:31.483+05:30 level=INFO source=types.go:42 msg=\"inference compute\" id=GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 filter_id=\"\" library=CUDA compute=8.0 name=CUDA0 description=\"NVIDIA A100-PCIE-40GB\" libdirs=ollama,cuda_v12 driver=12.2 pci_id=0000:e1:00.0 type=discrete total=\"40.0 GiB\" available=\"39.4 GiB\"\n",
      "Attempt 2/8 to call Ollama...\n",
      "time=2025-12-11T18:09:41.423+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --port 39757\"\n",
      "time=2025-12-11T18:09:42.096+05:30 level=INFO source=server.go:209 msg=\"enabling flash attention\"\n",
      "time=2025-12-11T18:09:42.096+05:30 level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/home/ajit_2411ai19/opt/ollama/bin/ollama runner --ollama-engine --model /home/ajit_2411ai19/.ollama/models/blobs/sha256-e7b273f9636059a689e3ddcab3716e4f65abe0143ac978e46673ad0e52d09efb --port 36547\"\n",
      "time=2025-12-11T18:09:42.097+05:30 level=INFO source=sched.go:443 msg=\"system memory\" total=\"251.7 GiB\" free=\"225.0 GiB\" free_swap=\"9.3 GiB\"\n",
      "time=2025-12-11T18:09:42.097+05:30 level=INFO source=sched.go:450 msg=\"gpu memory\" id=GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 library=CUDA available=\"38.9 GiB\" free=\"39.4 GiB\" minimum=\"457.0 MiB\" overhead=\"0 B\"\n",
      "time=2025-12-11T18:09:42.097+05:30 level=INFO source=server.go:702 msg=\"loading model\" \"model layers\"=25 requested=-1\n",
      "time=2025-12-11T18:09:42.127+05:30 level=INFO source=runner.go:1398 msg=\"starting ollama engine\"\n",
      "time=2025-12-11T18:09:42.127+05:30 level=INFO source=runner.go:1433 msg=\"Server listening on 127.0.0.1:36547\"\n",
      "time=2025-12-11T18:09:42.132+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T18:09:42.258+05:30 level=INFO source=ggml.go:136 msg=\"\" architecture=gptoss file_type=MXFP4 name=\"\" description=\"\" num_tensors=459 num_key_values=32\n",
      "load_backend: loaded CPU backend from /home/ajit_2411ai19/opt/ollama/lib/ollama/libggml-cpu-haswell.so\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100-PCIE-40GB, compute capability 8.0, VMM: yes, ID: GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14\n",
      "load_backend: loaded CUDA backend from /home/ajit_2411ai19/opt/ollama/lib/ollama/cuda_v12/libggml-cuda.so\n",
      "time=2025-12-11T18:09:42.478+05:30 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-12-11T18:09:43.247+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T18:09:43.480+05:30 level=INFO source=runner.go:1271 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:96 GPULayers:25[ID:GPU-a4e88315-0bff-eaf1-6f4b-11d8383d2e14 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-11T18:09:43.481+05:30 level=INFO source=ggml.go:482 msg=\"offloading 24 repeating layers to GPU\"\n",
      "time=2025-12-11T18:09:43.481+05:30 level=INFO source=ggml.go:489 msg=\"offloading output layer to GPU\"\n",
      "time=2025-12-11T18:09:43.481+05:30 level=INFO source=ggml.go:494 msg=\"offloaded 25/25 layers to GPU\"\n",
      "time=2025-12-11T18:09:43.481+05:30 level=INFO source=device.go:240 msg=\"model weights\" device=CUDA0 size=\"11.8 GiB\"\n",
      "time=2025-12-11T18:09:43.481+05:30 level=INFO source=device.go:245 msg=\"model weights\" device=CPU size=\"1.1 GiB\"\n",
      "time=2025-12-11T18:09:43.481+05:30 level=INFO source=device.go:251 msg=\"kv cache\" device=CUDA0 size=\"300.0 MiB\"\n",
      "time=2025-12-11T18:09:43.481+05:30 level=INFO source=device.go:262 msg=\"compute graph\" device=CUDA0 size=\"178.6 MiB\"\n",
      "time=2025-12-11T18:09:43.481+05:30 level=INFO source=device.go:267 msg=\"compute graph\" device=CPU size=\"5.6 MiB\"\n",
      "time=2025-12-11T18:09:43.481+05:30 level=INFO source=device.go:272 msg=\"total memory\" size=\"13.3 GiB\"\n",
      "time=2025-12-11T18:09:43.481+05:30 level=INFO source=sched.go:517 msg=\"loaded runners\" count=1\n",
      "time=2025-12-11T18:09:43.481+05:30 level=INFO source=server.go:1294 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-12-11T18:09:43.482+05:30 level=INFO source=server.go:1328 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "time=2025-12-11T18:09:46.744+05:30 level=INFO source=server.go:1332 msg=\"llama runner started in 4.65 seconds\"\n",
      "[GIN] 2025/12/11 - 18:10:08 |\u001b[97;42m 200 \u001b[0m| 27.943303301s |       127.0.0.1 |\u001b[97;46m POST    \u001b[0m \"/api/generate\"\n",
      "Ollama generated the response successfully.\n",
      "\n",
      "‚ú® FINAL VERIFICATION RESULTS:\n",
      "   [Retrieval] Old Rank: 10 -> New Rank: 2\n",
      "   [Generative] Old Vis: 0.0 -> New Vis: 2.0117\n",
      "   Verdict: üèÜ GEO SUCCESS! Visibility Increased.\n",
      "   üíæ Saved verification to data/verification_result.json\n"
     ]
    }
   ],
   "source": [
    "!python3 verify_optimization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c915e9e8-f704-49a0-a131-e16584f638d4",
   "metadata": {},
   "source": [
    "---\n",
    "## Maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f66a59-6c82-4281-b498-5fb688f758de",
   "metadata": {},
   "source": [
    "---\n",
    "**In case LLM runs on CPU,** run the following to select the available GPU.\n",
    "\n",
    "Run the following to see detailed usage of GPU \n",
    "```powershell\n",
    "ps -eo user,pid,%cpu,%mem,cmd --sort=-%mem\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ad707e6-d1eb-41c1-973f-ae6b5e8ddbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=4\n",
      "echo \"Using GPU 4 with 40322 MiB free\"\n"
     ]
    }
   ],
   "source": [
    "!python pick_gpu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "383caa72-c8b7-48a8-a6b3-cf6990fa6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bab0f7c-dc53-4fec-8f17-f26595a7fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b9fd957-6d29-44a7-8d80-c700951c67f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53907ba8-e681-4aca-b733-0e096845a8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
