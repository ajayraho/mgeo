import pandas as pd
import numpy as np
import torch
from sentence_transformers import SentenceTransformer, util
import os
import argparse  # Added for command line arguments

# --- CONFIGURATION ---
MODEL_NAME = 'BAAI/bge-large-en-v1.5'
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
BATCH_SIZE = 128

class LocalSearchEngine:
    def __init__(self, dataframe, cache_file, force_refresh=False):
        """
        Initializes the Search Engine with Persistent Caching.
        
        Args:
            dataframe: The pd.DataFrame containing product data.
            cache_file: Path to the .pt file where vectors will be stored.
            force_refresh: If True, ignores existing cache and re-computes.
        """
        self.cache_file = cache_file # Store the specific cache path for this instance
        print(f"ğŸš€ Initializing Search Engine on {DEVICE}...")
        self.df = dataframe.copy().reset_index(drop=True)
        self.model = None 
        
        # 1. Try to Load from Disk
        if os.path.exists(self.cache_file) and not force_refresh:
            print(f"   ğŸ“‚ Found cached index '{self.cache_file}'. Loading...")
            try:
                self.load_index()
                print("   âœ… Loaded from disk successfully.")
                return 
            except Exception as e:
                print(f"   âš ï¸ Cache load failed ({e}). Re-computing...")
        
        # 2. Compute if not cached
        print(f"   âš™ï¸ Computing new vector index (Model: {MODEL_NAME})...")
        self.model = SentenceTransformer(MODEL_NAME, device=DEVICE)
        
        self._create_search_payload()
        
        print(f"   Vectorizing {len(self.df)} products...")
        embeddings_cpu = self.model.encode(
            self.df['search_payload'].tolist(),
            batch_size=BATCH_SIZE,
            show_progress_bar=True,
            normalize_embeddings=True,
            convert_to_tensor=True,
            device=DEVICE
        )
        
        self.embeddings = embeddings_cpu
        self.save_index()
        print(f"   ğŸ’¾ Index saved to disk at: {self.cache_file}")
        print("âœ… Search Engine Online.")

    def _create_search_payload(self):
        def clean(x): return str(x).strip() if pd.notna(x) else ""
        
        # Ensure columns exist to prevent crash on arbitrary CSVs
        for col in ['title', 'features']:
            if col not in self.df.columns:
                self.df[col] = ""

        instruction = "Represent this product document for retrieval: "
        self.df['search_payload'] = (
            instruction + 
            # "Category: " + self.df['category'].apply(clean) + " ; " +
            "Title: " + self.df['title'].apply(clean) + " ; " +
            "Description: " + self.df['features'].apply(clean)
        )

    def save_index(self):
        """Saves to disk. Moves to CPU to avoid pickling GPU tensors."""
        payload = {
            'embeddings': self.embeddings.cpu(),
            'dataframe': self.df
        }
        torch.save(payload, self.cache_file)

    def load_index(self):
        """Loads from disk. Fixes PyTorch 2.6+ security error."""
        # weights_only=False is required to load Pandas DataFrames
        payload = torch.load(self.cache_file, map_location=DEVICE, weights_only=False)
        self.embeddings = payload['embeddings'].to(DEVICE)
        self.df = payload['dataframe']

    def search(self, query, top_k=5):
        """Performs Pure Semantic Search (No Hard Filters)."""
        if self.model is None:
            self.model = SentenceTransformer(MODEL_NAME, device=DEVICE)

        instruction = "Represent this sentence for searching relevant passages: "
        
        with torch.no_grad():
            query_vec = self.model.encode(
                [instruction + query], 
                normalize_embeddings=True, 
                convert_to_tensor=True, 
                device=DEVICE
            )
            
            scores = util.dot_score(query_vec, self.embeddings)[0]
            top_results = torch.topk(scores, k=top_k)
            
            top_indices = top_results.indices.cpu().numpy()
            top_scores = top_results.values.cpu().numpy()
        
        results_df = self.df.iloc[top_indices].copy()
        results_df['relevance_score'] = top_scores
        return results_df

    def format_for_rag(self, results_df):
        context_str = ""
        for i, (idx, row) in enumerate(results_df.iterrows()):
            context_str += f"""
[Result #{i+1} | ID: {row.get('item_id', 'N/A')}]
Category: {row.get('category', 'N/A')}
Title: {row.get('title', 'N/A')}
Features: {str(row.get('features', 'N/A'))}...
Score: {row['relevance_score']:.4f}
--------------------------------------------------
"""
        return context_str

# --- MAIN EXECUTION ---
if __name__ == "__main__":
    # 1. Setup Argument Parser
    parser = argparse.ArgumentParser(description="Embed a CSV dataset for Semantic Search.")
    parser.add_argument("csv_path", type=str, help="Path to the input CSV file.")
    
    args = parser.parse_args()
    input_csv = args.csv_path
    
    # 2. Determine Output Filename (.csv -> .pt)
    base, ext = os.path.splitext(input_csv)
    output_pt = base + ".pt"
    
    if not os.path.exists(input_csv):
        print(f"âŒ Error: File '{input_csv}' not found.")
        exit(1)

    print(f"ğŸ“‚ Reading CSV: {input_csv}")
    print(f"ğŸ’¾ Target Cache: {output_pt}")

    # 3. Load Data
    try:
        df = pd.read_csv(input_csv)
        df = df[['title', 'features', 'brand']]
    except Exception as e:
        print(f"âŒ Failed to read CSV: {e}")
        exit(1)

    # 4. Initialize Engine (Computes and Saves .pt file)
    engine = LocalSearchEngine(df, cache_file=output_pt)

    # 5. Quick Test
    print("\nğŸ” Running Sanity Check (Query: 'quality product')...")
    results = engine.search("quality product", top_k=10)
    print(engine.format_for_rag(results))